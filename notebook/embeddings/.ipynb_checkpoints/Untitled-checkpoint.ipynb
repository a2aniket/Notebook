{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddcdb2f2-d7f0-4764-b69f-313fc9cd6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"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\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"langchain-project\"\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "EMBEDDING_PATH = \"C:/code/qp-ai-assessment/api/misc/embeddings/\"\n",
    "MODEL_KWARGS = {\"device\": \"cpu\"}\n",
    "ENCODE_KWARGS = {\"normalize_embeddings\": True}\n",
    "\n",
    "\n",
    "\n",
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs=MODEL_KWARGS,\n",
    "    encode_kwargs=ENCODE_KWARGS\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68fe1365-82a9-41cd-90d1-b301e60d019b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset name\n",
    "from langsmith import Client\n",
    "\n",
    "# Clone dataset\n",
    "client = Client()\n",
    "dataset = client.clone_public_dataset(\n",
    "    \"https://smith.langchain.com/public/730d833b-74da-43e2-a614-4e2ca2502606/d\"\n",
    ")\n",
    "\n",
    "dataset_name = \"LCEL-QA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6213e86-8cf5-438c-87c6-f8623876240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INDEX\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load docs\n",
    "url = \"https://python.langchain.com/v0.1/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4500, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed and store in Chroma\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=huggingface_embeddings)\n",
    "\n",
    "# Index\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22f44183-b669-402d-95a1-934cb48f40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.schema import Document\n",
    "from langsmith import traceable\n",
    "\n",
    "class RagBot:\n",
    "    def __init__(self, retriever, repo_id: str = \"mistralai/Mistral-7B-v0.1\"):\n",
    "        self._retriever = retriever\n",
    "        self._llm = HuggingFaceHub(\n",
    "            repo_id=repo_id,\n",
    "            model_kwargs={\"temperature\": 0.1, \"max_length\": 500}\n",
    "        )\n",
    "\n",
    "    @traceable()\n",
    "    def retrieve_docs(self, question):\n",
    "        return self._retriever.get_relevant_documents(question)\n",
    "\n",
    "    @traceable()\n",
    "    def invoke_llm(self, question: str, docs: List[Document]):\n",
    "        docs_content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        system_prompt = (\n",
    "            \"You are a helpful AI code assistant with expertise in LCEL. \"\n",
    "            \"Use the following docs to produce a concise code solution to the user question.\\n\\n\"\n",
    "            f\"## Docs\\n\\n{docs_content}\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        input_text = f\"{system_prompt}Human: {question}\\nAssistant:\"\n",
    "        \n",
    "        response = self._llm(input_text)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response,\n",
    "            \"contexts\": [doc.page_content for doc in docs],\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def get_answer(self, question: str):\n",
    "        docs = self.retrieve_docs(question)\n",
    "        return self.invoke_llm(question, docs)\n",
    "\n",
    "# Initialize the RagBot with a retriever\n",
    "rag_bot = RagBot(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bca2f47a-4055-44d8-81a2-9ff0f017ac42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a helpful AI code assistant with expertise in LCEL. Use the following docs to produce a concise code solution to the user question.\\n\\n## Docs\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_bot.get_answer(\"How to build a RAG chain in LCEL?\")\n",
    "response[\"answer\"][:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dfb61a6-7a7d-480a-9769-043d9dae7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"input_question\"])\n",
    "    return {\"answer\": response[\"answer\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09878c27-4511-41e8-9bbf-d5ca2e86486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"input_question\"]\n",
    "    correct_answer = example.outputs[\"output_answer\"]\n",
    "    student_answer = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "        temperature=0.1,\n",
    "        model_kwargs={\"max_length\": 500}  # Use model_kwargs instead of max_length\n",
    "    )\n",
    "\n",
    "    # Use the StructuredPrompt directly\n",
    "    grading_chain = LLMChain(llm=llm, prompt=grade_prompt_answer_accuracy)\n",
    "\n",
    "    # Run evaluator\n",
    "    result = grading_chain.run(\n",
    "        question=input_question,\n",
    "        correct_answer=correct_answer,\n",
    "        student_answer=student_answer\n",
    "    )\n",
    "\n",
    "    # Parse the result to extract the score\n",
    "    # This assumes the model outputs the score in a format like \"Score: 8\"\n",
    "    try:\n",
    "        score = float(result.split(\"Score:\")[-1].strip().split()[0])\n",
    "    except:\n",
    "        score = 0  # Default score if parsing fails\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68e93a6e-779e-4719-82a0-61478eaf5697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-v-reference-df3b5ffb' at:\n",
      "https://smith.langchain.com/o/b59f3614-9d00-5c85-a713-5c5f2675f256/datasets/bd1b4c43-df52-4967-aa0a-2ff72b00592d/compare?selectedSessions=7801917f-6774-47db-b3a2-730b5751a26f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dfef87c4a1479a813078b64590cbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Token is valid (permission: read).\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\thora\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"rag-answer-v-reference\",\n",
    "    metadata={\"version\": \"LCEL context, mistralai/Mistral-7B-v0.1\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c4ef7-11a0-47e5-a04d-13f08273e724",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9a608c-b790-401f-bb7c-dabcdf81c407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
